{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":370},"executionInfo":{"elapsed":74603,"status":"error","timestamp":1712322278181,"user":{"displayName":"Mariam Sayed Ahmad","userId":"02593307766313333102"},"user_tz":-180},"id":"pdsIsTKIRxMS","outputId":"f808a02b-2b7d-4599-9338-d2e0f947e60a"},"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-da0d71898a3e>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"'/content/drive/MyDrive/Capstone Project/Acumen_OD_Classification/'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mHOME\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    131\u001b[0m   )\n\u001b[1;32m    132\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# @title Mounting Google Drive\n","import os\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","%cd '/content/drive/MyDrive/Capstone Project/Acumen_OD_Classification/'\n","HOME = os.getcwd()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":275,"status":"ok","timestamp":1701957061828,"user":{"displayName":"Mahmoud Nasser","userId":"17520528083434567620"},"user_tz":-120},"id":"-I8kKlPz9W7Q","outputId":"3b71dc60-53c0-4b0c-c4ec-dde4e4b65b0d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['Shelf', 'Desk', 'Stool', 'Bench', 'Table', 'Lamp', 'Chair',\n","       'Board', 'Mirror'], dtype=object)"]},"metadata":{},"execution_count":35}],"source":["#@title Load database\n","import pandas as pd\n","data_path = '/content/drive/MyDrive/Capstone Project/Acumen_OD_Classification/dataframeAcumenFinal.csv'\n","df_acumen = pd.read_csv(data_path)\n","#df_acumen.info()\n","df_acumen['manufacturer2'].unique()\n","#df_acumen['category'].unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"KN-PCnDYzLA4"},"outputs":[],"source":["# @title importing supervision library\n","!pip install -q supervision\n","import supervision as sv"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39684,"status":"ok","timestamp":1701957108036,"user":{"displayName":"Mahmoud Nasser","userId":"17520528083434567620"},"user_tz":-120},"id":"8er1uLGdSQvy","outputId":"1d41d217-f5ca-486e-e8e7-1fcb2c9b86f2"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Capstone Project/Acumen_OD_Classification/GroundingDINO\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","/content/drive/MyDrive/Capstone Project/Acumen_OD_Classification/weights\n","/content/drive/MyDrive/Capstone Project/Acumen_OD_Classification/GroundingDINO\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/MyDrive/Capstone Project/Acumen_OD_Classification/GroundingDINO/groundingdino/models/GroundingDINO/ms_deform_attn.py:31: UserWarning: Failed to load custom C++ ops. Running on CPU mode Only!\n","  warnings.warn(\"Failed to load custom C++ ops. Running on CPU mode Only!\")\n","/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"]},{"output_type":"stream","name":"stdout","text":["final text_encoder_type: bert-base-uncased\n","/content/drive/MyDrive/Capstone Project/Acumen_OD_Classification\n"]}],"source":["# @title loading Grounding DINO\n","#!git clone https://github.com/IDEA-Research/GroundingDINO.git\n","%cd {HOME}/GroundingDINO\n","!pip install -q -e .\n","\n","\n","CONFIG_PATH = os.path.join(HOME, \"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\")\n","%cd {HOME}/weights\n","#!wget -q https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth\n","WEIGHTS_NAME = \"groundingdino_swint_ogc.pth\"\n","WEIGHTS_PATH = os.path.join(HOME, \"weights\", WEIGHTS_NAME)\n","%cd {HOME}/GroundingDINO\n","\n","\n","from groundingdino.util.inference import load_model, load_image, predict, annotate\n","GDINO = load_model(CONFIG_PATH, WEIGHTS_PATH)\n","%cd {HOME}"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"degVJkYgC0vs"},"outputs":[],"source":["# @title loading CLIP\n","from transformers import CLIPProcessor, CLIPImageProcessor, CLIPModel, CLIPPreTrainedModel\n","model_ID = \"openai/clip-vit-base-patch32\"\n","ICLIP = CLIPModel.from_pretrained(model_ID)\n","preprocess = CLIPImageProcessor.from_pretrained(model_ID)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"ZXSJe_WmvKfO"},"outputs":[],"source":["# @title necessary libraries\n","import torch\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","import matplotlib.image as mpimg\n","from PIL import Image\n","import os\n","import requests\n","import io\n","import numpy as np\n","from torchvision import transforms\n","import pandas as pd\n","from io import BytesIO\n","import re\n","import json"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"PAu81aHEz0Dq"},"outputs":[],"source":["# @title OD(image_source, text_prompt, threshold)\n","def OD(image_source, text_prompt, threshold):\n","    normalized_array = image_source / 255.0\n","    transposed_array = normalized_array.transpose((2, 0, 1))\n","    image_tensor = torch.tensor(transposed_array, dtype=torch.float32)\n","\n","    boxes, logits, phrases = predict(\n","        device='cpu',\n","        model=GDINO,\n","        image=image_tensor,\n","        caption=text_prompt,\n","        box_threshold=threshold,\n","        text_threshold=threshold\n","    )\n","\n","    unique_phrases = set(phrases)  # Get unique phrases\n","\n","    filtered_boxes = []\n","    filtered_logits = []\n","    filtered_phrases = []\n","\n","    for phrase in unique_phrases:\n","        # Find indices where the phrase occurs\n","        indices = [i for i, p in enumerate(phrases) if p == phrase]\n","\n","        # Find the index with the highest logits\n","        max_index = max(indices, key=lambda i: logits[i])\n","\n","        # Add the item with the highest logits to the filtered lists\n","        filtered_boxes.append(boxes[max_index])\n","        filtered_logits.append(logits[max_index])\n","        filtered_phrases.append(phrases[max_index])\n","\n","    return filtered_boxes, filtered_logits, filtered_phrases"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"SRwCbb5s0U8H"},"outputs":[],"source":["# @title plot_boxes(image_url, text_prompt_list, threshold)\n","def plot_boxes(image_url, text_prompt_list, threshold):\n","    image_array = url_to_image_array(image_url)\n","    image = Image.fromarray(image_array)\n","    img_width, img_height = image.size\n","\n","    fig, ax = plt.subplots(1, figsize=(16, 12))\n","    ax.set_axis_off()\n","\n","    ax.imshow(image)\n","\n","    for text_prompt in text_prompt_list:\n","        boxes, logits, phrases = OD(image_array, text_prompt, threshold)\n","\n","        for i, box in enumerate(boxes):\n","            x_center, y_center, width, height = box.tolist()\n","            x_min = max(0, (x_center - width / 2) * img_width)\n","            y_min = max(0, (y_center - height / 2) * img_height)\n","            x_max = min(img_width, (x_center + width / 2) * img_width)\n","            y_max = min(img_height, (y_center + height / 2) * img_height)\n","\n","            rect = patches.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=3, edgecolor='r', facecolor='none')\n","\n","            ax.add_patch(rect)\n","\n","            phrase = phrases[i]\n","            logit = logits[i]\n","            logit_percentage = int(logits[i] * 100)\n","            ax.text(x_min, y_min - 5, f\"{phrase} ({logit_percentage:.0f}%)\", color='r', fontsize=15, ha='left', va='bottom')\n","\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"e127SCTb0g9L"},"outputs":[],"source":["# @title plot_dots(image_url, text_prompt_list, threshold)\n","def plot_dots(image_url, text_prompt_list, threshold):\n","    image_array = url_to_image_array(image_url)\n","    image = Image.fromarray(image_array)\n","    img_width, img_height = image.size\n","\n","    fig, ax = plt.subplots(1, figsize=(16, 12))\n","    ax.set_axis_off()\n","\n","    ax.imshow(image)\n","\n","    for text_prompt in text_prompt_list:\n","        boxes, logits, phrases = OD(image_array, text_prompt, threshold)\n","        for i, box in enumerate(boxes):\n","            x_center, y_center, width, height = box.tolist()\n","            x_min = max(0, (x_center - width / 2) * img_width)\n","            y_min = max(0, (y_center - height / 2) * img_height)\n","            x_max = min(img_width, (x_center + width / 2) * img_width)\n","            y_max = min(img_height, (y_center + height / 2) * img_height)\n","            x = x_center * img_width\n","            y = y_center * img_height\n","\n","            dot_x = x\n","            dot_y = y_min\n","\n","            ax.plot(dot_x, dot_y, marker='o', markersize=12, color='red', label='Dot')\n","\n","            phrase = phrases[i]\n","            logit = logits[i]\n","            logit_percentage = int(logits[i] * 100)\n","            ax.text(dot_x - 50, dot_y + 10, f\"{phrase} ({logit_percentage:.0f}%)\", color='red', fontsize=15)\n","\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"nhmINKohrBqD"},"outputs":[],"source":["# @title get_ROI_images(image_source, text_prompt, threshold)\n","def get_ROI_images(image_source, text_prompt, threshold):\n","    detected_objects = []\n","    image = Image.fromarray(image_source).convert('RGB')\n","    img_width, img_height = image.size\n","    bounding_boxes, accuracies, labels = OD(image_source, text_prompt, threshold)\n","\n","    for i, box in enumerate(bounding_boxes):\n","        x_center, y_center, width, height = box.tolist()\n","        x_min = max(0, (x_center - width / 2) * img_width)\n","        y_min = max(0, (y_center - height / 2) * img_height)\n","        x_max = min(img_width, (x_center + width / 2) * img_width)\n","        y_max = min(img_height, (y_center + height / 2) * img_height)\n","        detected_object = image.crop((x_min, y_min, x_max, y_max))\n","        detected_object = detected_object.resize((img_width, img_height), Image.ANTIALIAS)\n","        img_array = np.array(detected_object)\n","        detected_objects.append(img_array)\n","\n","    return detected_objects, labels"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"2ST0xSpQxjFU"},"outputs":[],"source":["#@title get_ROI_images_and_boxes(image_source, text_prompt, threshold)\n","def get_ROI_images_and_boxes(image_source, text_prompt, threshold):\n","    detected_objects = []\n","    image = Image.fromarray(image_source).convert('RGB')\n","    img_width, img_height = image.size\n","    bounding_boxes, accuracies, labels = OD(image_source, text_prompt, threshold)\n","\n","    for i, box in enumerate(bounding_boxes):\n","        x_center, y_center, width, height = box.tolist()\n","        x_min = max(0, (x_center - width / 2) * img_width)\n","        y_min = max(0, (y_center - height / 2) * img_height)\n","        x_max = min(img_width, (x_center + width / 2) * img_width)\n","        y_max = min(img_height, (y_center + height / 2) * img_height)\n","        x = x_center * img_width\n","        y = y_min\n","        width_ = width * img_width\n","        height_ = height * img_height\n","        detected_object = image.crop((x_min, y_min, x_max, y_max))\n","        detected_object = detected_object.resize((img_width, img_height), Image.ANTIALIAS)\n","        img_array = np.array(detected_object)\n","        detected_objects.append((img_array, (x, y, width_, height_)))\n","\n","    return detected_objects, labels"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"QAgvALs5pYEv"},"outputs":[],"source":["# @title image_load_and_preprocess(image)\n","def image_load_and_preprocess(image):\n","    image = image.convert(\"RGB\")\n","    image = preprocess(image, return_tensors=\"pt\")\n","    return image\n","\n","def path_load_and_preprocess(image_path):\n","    image = Image.open(image_path)\n","    image = image.convert(\"RGB\")\n","    image = preprocess(image, return_tensors=\"pt\")\n","    return image"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"Fu9QiJfcmEz-"},"outputs":[],"source":["# @title convert_numpy_to_image(numpy_images)\n","def convert_numpy_to_image(numpy_images):\n","    image_objects = []\n","\n","    for numpy_img in numpy_images:\n","        image_obj = Image.fromarray(np.uint8(numpy_img))\n","        image_objects.append(image_obj)\n","\n","    return image_objects"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"jemIt3mCobgP"},"outputs":[],"source":["# @title check_labels(filename, label_list)\n","def check_labels(filename, label_list):\n","    cleaned_name = re.sub(r'[^a-zA-Z\\s]', '', filename)\n","    combined_name = ' '.join(cleaned_name.split())\n","    combined_labels = ' '.join(label_list)\n","\n","    words_set_1 = set(combined_name.lower().split())\n","    words_set_2 = set(combined_labels.lower().split())\n","\n","    matched_words = words_set_1.intersection(words_set_2)\n","    #if len(matched_words)>0:\n","      #print(f\"Matched words: {', '.join(matched_words)}\")\n","    return any(word in words_set_2 for word in words_set_1)\n","\n","\n","def check_labels_v2(filename, label_list):\n","    return [label for label in label_list if filename.lower() == label.lower()]"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"WYE9vvkRI1bc"},"outputs":[],"source":["# @title url_to_image_array(url)\n","def url_to_image_array(url):\n","\n","    response = requests.get(url)\n","    image_ac = Image.open(BytesIO(response.content))\n","    image_ac = image_ac.convert('RGB')\n","    image_array = np.array(image_ac)\n","\n","    return image_array"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"EM7OdQ69tNqa"},"outputs":[],"source":["#@title transform_image(image)\n","def transform_image(image):\n","    aspect_ratio = image.width / image.height\n","\n","    if image.width > image.height:\n","        new_width = 240\n","        new_height = int(120 / aspect_ratio)\n","    else:\n","        new_width = int(240 * aspect_ratio)\n","        new_height = 120\n","\n","    transform = transforms.Compose([\n","        transforms.Resize((new_height, new_width)),\n","        transforms.CenterCrop(240)\n","    ])\n","\n","    return transform(image)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fEYgNTWs5OWj","cellView":"form"},"outputs":[],"source":["#@title process_images_and_get_scores(url, text_prompt_list, dataframe, threshold)\n","def process_images_and_get_scores(url, text_prompt_list, dataframe, threshold):\n","    similarity_model = ICLIP\n","\n","    image_array = url_to_image_array(url)\n","\n","    images_a_arrays_list = []\n","    labels_list = []\n","    similarity_scores_list_list = []\n","    id_values_list_list = []\n","\n","    for prompt in text_prompt_list:\n","        images_a_arrays, labels = get_ROI_images(image_source=image_array, text_prompt=prompt, threshold=threshold)\n","\n","        image_objects = convert_numpy_to_image(images_a_arrays)\n","        images_a = [im for im in image_objects]\n","        c = 0\n","        similarity_scores_list = []\n","        id_values_list = []\n","\n","        for index_l, image_a in enumerate(images_a):\n","            image_a = transform_image(image_a)\n","            image_a = image_load_and_preprocess(image_a)[\"pixel_values\"]\n","\n","            with torch.no_grad():\n","                embedding_a = similarity_model.get_image_features(image_a)\n","            embedding_a = torch.nn.functional.normalize(embedding_a, p=2, dim=1)\n","\n","            similarity_scores = []\n","\n","            for idx, image_b_url in enumerate(dataframe['image']):\n","                if check_labels(dataframe.loc[idx, 'manufacturer2'], labels):\n","                    c += 1\n","                    response = requests.get(image_b_url)\n","                    image_b_source = Image.open(BytesIO(response.content))\n","                    image_b = image_b_source.convert('RGB')\n","\n","                    image_b = transform_image(image_b)\n","                    image_b = image_load_and_preprocess(image_b)[\"pixel_values\"]\n","\n","                    with torch.no_grad():\n","                        embedding_b = similarity_model.get_image_features(image_b)\n","                    embedding_b = torch.nn.functional.normalize(embedding_b, p=2, dim=1)\n","\n","                    similarity_score = torch.nn.functional.cosine_similarity(embedding_a, embedding_b)\n","\n","                    similarity_scores.append((similarity_score.item(), image_b_source, dataframe.loc[idx, 'id']))\n","\n","            similarity_scores.sort(reverse=True, key=lambda x: x[0])\n","            similarity_scores = [score_tuple for score_tuple in similarity_scores if score_tuple[0] >= 0.75]\n","            similarity_scores = similarity_scores[:4]\n","            similarity_scores_list.append(similarity_scores)\n","\n","        print(f'Number of fetched rows for prompt \"{prompt}\": {c}')\n","        images_a_arrays_list.append(images_a_arrays)\n","        labels_list.append(labels)\n","        similarity_scores_list_list.append(similarity_scores_list)\n","\n","    return images_a_arrays_list, labels_list, similarity_scores_list_list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3kKPXaIsSznu","cellView":"form"},"outputs":[],"source":["# @title plot_similarity_scores(images_a_arrays_list, labels_list, similarity_scores_list_list, text_prompt_list)\n","def plot_similarity_scores(images_a_arrays_list, labels_list, similarity_scores_list_list, text_prompt_list):\n","    for prompt, images_a_arrays, labels, similarity_scores_list in zip(text_prompt_list, images_a_arrays_list, labels_list, similarity_scores_list_list):\n","\n","        for idx, (image_a, similarity_scores) in enumerate(zip(images_a_arrays, similarity_scores_list)):\n","            num_images_to_plot = min(6, len(similarity_scores))\n","\n","            fig_input, axes_input = plt.subplots(1, 7, figsize=(20, 3))\n","            fig_input.suptitle(f'Input Image {labels[idx]} ({prompt})')\n","            img_input = image_a\n","            axes_input[0].imshow(img_input)\n","            axes_input[0].set_title('Input Image')\n","            axes_input[0].axis('off')\n","\n","            for i in range(num_images_to_plot):\n","                img = similarity_scores[i][1]\n","                axes_input[i + 1].imshow(img)\n","                axes_input[i + 1].set_title(f\"Similarity: {similarity_scores[i][0] * 100:.0f}%\\nID: {similarity_scores[i][2]}\")\n","                axes_input[i + 1].axis('off')\n","\n","            plt.tight_layout()\n","            plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"nxkX8vUGx065"},"outputs":[],"source":["# @title get_outputs(url, text_prompt_list, dataframe, detection_model, similarity_model, threshold)\n","def get_outputs(url, text_prompt_list, dataframe, threshold):\n","    similarity_model = ICLIP\n","\n","    image_array = url_to_image_array(url)\n","\n","    similarity_list = []\n","\n","    for prompt in text_prompt_list:\n","        detected_objects, labels = get_ROI_images_and_boxes(image_source=image_array, text_prompt=prompt, threshold=threshold)\n","\n","        for index_l, (image_a_array, bounding_box) in enumerate(detected_objects):\n","            x, y, width_, height_ = bounding_box\n","\n","\n","            image_obj = Image.fromarray(np.uint8(image_a_array))\n","            image_obj = transform_image(image_obj)\n","            image_a = image_load_and_preprocess(image_obj)[\"pixel_values\"]\n","            with torch.no_grad():\n","                embedding_a = similarity_model.get_image_features(image_a)\n","            embedding_a = torch.nn.functional.normalize(embedding_a, p=2, dim=1)\n","\n","            similarity_scores = []\n","\n","            for idx, image_b_url in enumerate(dataframe['image']):\n","                if check_labels(dataframe.loc[idx, 'manufacturer2'], labels):\n","                    response = requests.get(image_b_url)\n","                    image_b_source = Image.open(BytesIO(response.content))\n","                    image_b_source = image_b_source.convert('RGB')\n","\n","                    image_b_source = transform_image(image_b_source)\n","                    image_b_array = image_load_and_preprocess(image_b_source)[\"pixel_values\"]\n","\n","                    with torch.no_grad():\n","                        embedding_b = similarity_model.get_image_features(torch.tensor(image_b_array, dtype=torch.float32))\n","                    embedding_b = torch.nn.functional.normalize(embedding_b, p=2, dim=1)\n","\n","                    similarity_score = torch.nn.functional.cosine_similarity(embedding_a, embedding_b)\n","\n","                    similarity_scores.append((similarity_score.item(), image_b_source, dataframe.loc[idx, 'id']))\n","\n","            similarity_scores.sort(reverse=True, key=lambda x: x[0])\n","            similarity_scores = [score_tuple for score_tuple in similarity_scores if score_tuple[0] >= 0.80]\n","\n","            similarity_scores = similarity_scores[:4]\n","            product_matches = []\n","\n","            for similarity_score in similarity_scores:\n","                confidence, image_b_source, product_id = similarity_score\n","\n","                product_match = {\n","                    \"product_id\": str(product_id),\n","                    \"confidence\": str(confidence),\n","                    \"x\": str(x),\n","                    \"y\": str(y),\n","                    \"width\": str(width_),\n","                    \"height\": str(height_)\n","                }\n","\n","                product_matches.append(product_match)\n","\n","            similarity_list.append({\n","                'prompt': prompt,\n","                'images_a_array': image_a_array,\n","                'labels': labels,\n","                'similarity_scores': product_matches\n","            })\n","\n","    return url, image_array.shape[1], image_array.shape[0], similarity_list"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"Zo3WnPojx1Bj"},"outputs":[],"source":["# @title get_json_output(url, img_width, img_height, similarity_list)\n","def get_json_output(url, img_width, img_height, similarity_list):\n","    json_structure = []\n","\n","    for similarity_item in similarity_list:\n","        prompt = similarity_item['prompt']\n","        image_a_array = similarity_item['images_a_array']\n","        labels = similarity_item['labels']\n","        similarity_scores = similarity_item['similarity_scores']\n","\n","        product_matches = []\n","\n","        for similarity_score in similarity_scores:\n","            product_id = similarity_score['product_id']\n","            confidence = similarity_score['confidence']\n","            x, y, width, height = map(float, [similarity_score['x'], similarity_score['y'], similarity_score['width'], similarity_score['height']])\n","\n","            product_match = {\n","                \"product_id\": str(product_id),\n","                \"confidence\": str(confidence),\n","                \"x\": str(x),\n","                \"y\": str(y),\n","                \"width\": str(width),\n","                \"height\": str(height)\n","            }\n","\n","            product_matches.append(product_match)\n","\n","        json_item = {\n","            \"source_media\": {\n","                \"media_url\": url,\n","                \"width\": str(img_width),\n","                \"height\": str(img_height)\n","            },\n","            \"product_matches\": product_matches,\n","            \"tags\": [],\n","            \"errors\": []\n","        }\n","\n","        json_structure.append(json_item)\n","\n","    return json_structure"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"Cn5hcyxV0h1y"},"outputs":[],"source":["#@title output_json(url)\n","def output_json(url):\n","    threshold = 0.5\n","    data_path = '/content/drive/MyDrive/Capstone Project/Acumen_OD_Classification/dataframeAcumenFinal.csv'\n","    dataframe = pd.read_csv(data_path)\n","    text_prompt_list = ['chair',  'stool', 'table', 'shelf', 'desk', 'lamp', 'board', 'mirrior', 'bench']\n","    media_url, img_width, img_height, similarity_list = get_outputs(url, text_prompt_list, dataframe, threshold=threshold)\n","    output = get_json_output(media_url, img_width, img_height, similarity_list)\n","    return output"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"uNPlId6xS_dN"},"outputs":[],"source":["#@title output_plot(url)\n","def output_plot(url):\n","    threshold = 0.5\n","    data_path = '/content/drive/MyDrive/Capstone Project/Acumen_OD_Classification/dataframeAcumenFinal.csv'\n","    dataframe = pd.read_csv(data_path)\n","    text_prompt_list = ['chair',  'stool', 'table', 'shelf', 'desk', 'lamp', 'board', 'mirrior', 'bench']\n","    images_a_arrays_list, labels_list, similarity_scores_list_list = process_images_and_get_scores(url, text_prompt_list, dataframe, threshold)\n","    plot_similarity_scores(images_a_arrays_list, labels_list, similarity_scores_list_list, text_prompt_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j0D4HKFWQwt1"},"outputs":[],"source":["urls = [\n","    'https://app.oculizm.com/wp-content/uploads/2023/04/insta_profile_83953-155183964_471925360844077_6321490016420926228_n.jpg',\n","    'https://app.oculizm.com/wp-content/uploads/2023/04/insta_profile_83953-116957702_181330093377725_7625150121191732645_n-1024x976.jpg',\n","    'https://app.oculizm.com/wp-content/uploads/2023/04/insta_profile_83953-118980998_328984211683835_1353543039422003611_n-1024x1024.jpg',\n","    'https://app.oculizm.com/wp-content/uploads/2023/04/insta_profile_83953-104433654_119498016181098_7689914167204045686_n.jpg'\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R0lS_gl0nIR_"},"outputs":[],"source":["#output_plot(urls[0])\n","#output = output_json(urls[0])\n","#print(json.dumps(output, indent=2))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h12u3ZMk7Nqp","executionInfo":{"status":"ok","timestamp":1701955610866,"user_tz":-120,"elapsed":27473,"user":{"displayName":"Mahmoud Nasser","userId":"17520528083434567620"}},"outputId":"a0445fc6-24e0-4a50-d8e1-1fbebf11c068"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m104.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","lida 0.0.10 requires kaleido, which is not installed.\n","llmx 0.0.15a0 requires cohere, which is not installed.\n","llmx 0.0.15a0 requires openai, which is not installed.\n","llmx 0.0.15a0 requires tiktoken, which is not installed.\n","tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install -q gradio"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wEdWuhJU8LxX"},"outputs":[],"source":["import gradio as gr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1SK9ct-B0Nm6","cellView":"form"},"outputs":[],"source":["#@title process_images_gradio(image_input)\n","import numpy as np\n","def process_images_gradio(input_image):\n","    text_prompt_list = ['chair',  'stool', 'table', 'shelf', 'desk', 'lamp', 'board', 'mirrior', 'bench']\n","    dataframe = df_acumen\n","    threshold = 0.6\n","    similarity_model = ICLIP\n","\n","    image_array = np.array(input_image)\n","\n","    images_a_arrays_list = []\n","    labels_list = []\n","    similarity_scores_list_list = []\n","    id_values_list_list = []\n","    outputs_images_list = []\n","\n","    for prompt in text_prompt_list:\n","        images_a_arrays, labels = get_ROI_images(image_source=image_array, text_prompt=prompt, threshold=threshold)\n","\n","        image_objects = convert_numpy_to_image(images_a_arrays)\n","        images_a = [im for im in image_objects]\n","        similarity_scores_list = []\n","        id_values_list = []\n","\n","        for index_l, image_a in enumerate(images_a):\n","            image_a = transform_image(image_a)\n","            image_a = image_load_and_preprocess(image_a)[\"pixel_values\"]\n","\n","            with torch.no_grad():\n","                embedding_a = similarity_model.get_image_features(image_a)\n","            embedding_a = torch.nn.functional.normalize(embedding_a, p=2, dim=1)\n","\n","            similarity_scores = []\n","\n","            for idx, image_b_url in enumerate(dataframe['image']):\n","                if check_labels(dataframe.loc[idx, 'manufacturer2'], labels):\n","                    response = requests.get(image_b_url)\n","                    image_b_source = Image.open(BytesIO(response.content))\n","                    image_b = image_b_source.convert('RGB')\n","\n","                    image_b = transform_image(image_b)\n","                    image_b = image_load_and_preprocess(image_b)[\"pixel_values\"]\n","\n","                    with torch.no_grad():\n","                        embedding_b = similarity_model.get_image_features(image_b)\n","                    embedding_b = torch.nn.functional.normalize(embedding_b, p=2, dim=1)\n","\n","                    similarity_score = torch.nn.functional.cosine_similarity(embedding_a, embedding_b)\n","\n","                    similarity_scores.append((similarity_score.item(), image_b_source, dataframe.loc[idx, 'id']))\n","                    similarity_scores = [score_tuple for score_tuple in similarity_scores if score_tuple[0] >= 0.4]\n","\n","            similarity_scores.sort(reverse=True, key=lambda x: x[0])\n","            similarity_scores = similarity_scores[:4]\n","            similarity_scores_list.append(similarity_scores)\n","            outputs_images_list.append([(np.array(score_tuple[1]))/255.0 for score_tuple in similarity_scores])\n","\n","\n","        images_a_arrays_list.append(images_a_arrays)\n","        labels_list.append(labels)\n","        similarity_scores_list_list.append(similarity_scores_list)\n","\n","\n","    return outputs_images_list[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":611},"id":"7H_yLy8yKUik","executionInfo":{"status":"ok","timestamp":1701959573242,"user_tz":-120,"elapsed":5191,"user":{"displayName":"Mahmoud Nasser","userId":"17520528083434567620"}},"outputId":"b58a6167-7329-46f4-a9fe-b6c55c661e23"},"outputs":[{"output_type":"stream","name":"stdout","text":["Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","Running on public URL: https://aa97edb88f40281745.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://aa97edb88f40281745.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":132}],"source":["demo = gr.Interface(fn=process_images_gradio, inputs=gr.Image(width=600, height=400), outputs=[gr.Image(width=600, height=400), gr.Image(width=600, height=400), gr.Image(width=600, height=400), gr.Image(width=600, height=400)])\n","demo.launch(share=True)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}